{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of dogbreeds-cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOW+46lpdX3uk4x5LXme+Ju"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "62I2RlrPpAAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4_1aCmYBDTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorflow_version only exists in Colab.\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.ndimage as ndimage\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available: \", \n",
        "      len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "print(\"Tensorflow version: \", tf.__version__) \n",
        "\n",
        "\n",
        "#verbose = True   \n",
        "verbose = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DONjZ2kBMrG",
        "colab_type": "text"
      },
      "source": [
        "#Dataset Import and Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "26wYHGyqCtD-",
        "colab": {}
      },
      "source": [
        "np.random.seed(101)\n",
        "tf.random.set_seed(101)\n",
        "\n",
        "\n",
        "LABELS_FILE = \"/content/drive/My Drive/dogs/train_labels.csv\"\n",
        "\n",
        "df = pd.read_csv(LABELS_FILE)\n",
        "print(df.head())\n",
        "\n",
        "CLASS_NAMES = np.unique(df.breed)\n",
        "class_count = len(CLASS_NAMES)\n",
        "\n",
        "data_dir = \"/content/drive/My Drive/dogs/Dataset/train\"\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(\"Number of images:\", image_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXF2cCCKA04n",
        "colab_type": "text"
      },
      "source": [
        "#Image Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zchAXTnJ6LwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BATCH_SIZE = 32\n",
        "BATCH_SIZE = 200\n",
        "#BATCH_SIZE = 230\n",
        "IMG_HEIGHT = 331\n",
        "IMG_WIDTH = 331\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-iStQODC7qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a dataset of all files matching the pattern\n",
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
        "\n",
        "for f in list_ds.take(5):\n",
        "    print(f.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAb7l0qaJiS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label(file_path):\n",
        "    # convert the path to a list of path components\n",
        "    parts = tf.strings.split(file_path, os.path.sep)\n",
        "    # the second to last is the class-directory\n",
        "    return parts[-2] == CLASS_NAMES\n",
        "\n",
        "def decode_img(img):\n",
        "    # convert the compressed string to a 3D uint8 tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # resize the image to the desired size.\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    return img\n",
        "\n",
        "def process_path(file_path):\n",
        "    label = get_label(file_path)\n",
        "    # load raw data from the file as a string\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = decode_img(img)\n",
        "    return img, label  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFeidRhl9jJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply process_path function to each element of this dataset, and return a new\n",
        "# dataset containing the transformed elements, in the same order as they \n",
        "# appeared in the input.\n",
        "# set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "for image, label in labeled_ds.take(1):\n",
        "    print(\"Image shape: \", image.numpy().shape)\n",
        "    print(\"Label: \", label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPKksXiICO6t",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHNe0UvY553f",
        "colab_type": "text"
      },
      "source": [
        "## Data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtvfOw50EdxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def breed_distr(ds):\n",
        "    for breed in CLASS_NAMES:\n",
        "        count = 0\n",
        "        for element in ds.as_numpy_iterator(): \n",
        "            label = CLASS_NAMES[element[1] == 1][0]\n",
        "            if (breed in label):\n",
        "                count += 1\n",
        "        print(breed, count)\n",
        "\n",
        "\n",
        "def split_ds(ds, ds_size, split, cache=True, shuffle_buffer_size=1000):\n",
        "\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "    # cache after shuffle to get distinct elements in the split\n",
        "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "    # fit in memory.\n",
        "    if cache:\n",
        "        if isinstance(cache, str):\n",
        "            ds = ds.cache(cache)\n",
        "        else:\n",
        "            ds = ds.cache()\n",
        "  \n",
        "    val_size = np.floor(ds_size * split)\n",
        "    val_ds = ds.take(val_size)\n",
        "    train_ds = ds.skip(val_size)\n",
        "    \n",
        "    return (train_ds, val_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZUbSTdHCop0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds, val_ds = split_ds(labeled_ds, image_count, 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAciWWgoiONs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check class distribution\n",
        "if verbose:\n",
        "    breed_distr(train_ds)\n",
        "    breed_distr(val_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Sr0kw8ox2Q",
        "colab_type": "text"
      },
      "source": [
        "##Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2jaVV3NowIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_zoom(image, zoom_range, crop_size):\n",
        "    # apply zoom with 0.5 probability\n",
        "    if np.random.random() > 0.5:\n",
        "        min_zoom = zoom_range[0]\n",
        "        max_zoom = zoom_range[1]\n",
        "    \n",
        "        num_boxes = 1\n",
        "        boxes = np.zeros((num_boxes, 4))\n",
        "        scale = np.random.uniform(1.0 - max_zoom, 1.0 - min_zoom)\n",
        "        p = 0.5 - 0.5 * scale           # point on the diagonal\n",
        "        s = np.random.uniform(-p, p)    # diagonal shift\n",
        "        y_ul = x_ul = p + s             # upper left corner of the crop-box\n",
        "        y_lr = x_lr = 1 - p + s         # lower right corner of the crop-box\n",
        "        boxes[0] = [y_ul, x_ul, y_lr, x_lr]\n",
        "    \n",
        "        box_indices = np.zeros(num_boxes)\n",
        "        # crop and resize to the proper format\n",
        "        crops = tf.image.crop_and_resize([image], boxes, box_indices, crop_size)\n",
        "        image = crops[0]\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def process_image(image):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = random_zoom(image, zoom_range=(0.1, 0.4), \n",
        "                        crop_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    image = tf.image.random_brightness(image, max_delta=0.3)\n",
        "    image = tf.image.random_contrast(image, lower=0.5, upper=2)\n",
        "    image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n",
        "\n",
        "    return image, \n",
        "\n",
        "def tf_augment(image, label):\n",
        "    img_shape = image.shape\n",
        "    [image,] = tf.py_function(process_image, [image], [tf.float32])\n",
        "    image.set_shape(img_shape)\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6xzC0bnr81r",
        "colab_type": "text"
      },
      "source": [
        "## Training Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqCrfizv_uHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
        "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "    # fit in memory.\n",
        "    if cache:\n",
        "        if isinstance(cache, str):\n",
        "            ds = ds.cache(cache)\n",
        "        else:\n",
        "            ds = ds.cache()\n",
        "\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "    # Repeat forever\n",
        "    ds = ds.repeat()\n",
        "\n",
        "    # Apply augmentation\n",
        "    ds = ds.map(tf_augment, num_parallel_calls=AUTOTUNE)\n",
        "  \n",
        "    ds = ds.batch(BATCH_SIZE)\n",
        "  \n",
        "    # `prefetch` lets the dataset fetch batches in the background while the \n",
        "    # model is training.\n",
        "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  \n",
        "    return ds\n",
        "\n",
        "\n",
        "def show_batch(image_batch, label_batch):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for n in range(25):\n",
        "        ax = plt.subplot(5, 5, n + 1)\n",
        "        plt.imshow(image_batch[n])\n",
        "        plt.title(CLASS_NAMES[label_batch[n] == 1][0].title())\n",
        "        plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mueVAihiEwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batched_train_ds = prepare_for_training(train_ds).cache('./dogs-train.tfcache')\n",
        "batched_val_ds = val_ds.batch(BATCH_SIZE).cache('./dogs-val.tfcache')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iECB4zdOH4aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(batched_train_ds.element_spec)\n",
        "print(batched_val_ds.element_spec)\n",
        "\n",
        "if verbose:\n",
        "    image_batch, label_batch = next(batched_train_ds.as_numpy_iterator())\n",
        "    #image_batch, label_batch = next(batched_val_ds.as_numpy_iterator())\n",
        "    \n",
        "    show_batch(image_batch, label_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TOIQNjdHLfm",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhASHuTcUaz8",
        "colab_type": "text"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXm7MG-IUY1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model \n",
        "base_model = tf.keras.applications.NASNetLarge(input_shape=IMG_SHAPE,\n",
        "                                                     include_top=False,\n",
        "                                                     weights='imagenet')\n",
        "imagenet_weights = base_model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LnJVN0-Ut_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze the convolutional base (all the layers)\n",
        "# prevents the weights in a given layer from being updated during training\n",
        "base_model.trainable = False\n",
        "#base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwuY1kwtVIjE",
        "colab_type": "text"
      },
      "source": [
        "## Classification Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItUM2FIjHWNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_penalty = 0.001\n",
        "dropout_rate = 0.05\n",
        "\n",
        "global_average_layer = layers.GlobalAveragePooling2D()\n",
        "dropout_layer = layers.Dropout(dropout_rate)\n",
        "fc_layer = layers.Dense(1024, activation='relu', \n",
        "                        kernel_regularizer=regularizers.l2(l2_penalty))\n",
        "prediction_layer = layers.Dense(class_count, activation='softmax', \n",
        "                                kernel_regularizer=regularizers.l2(l2_penalty))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09soNbY6XhlF",
        "colab_type": "text"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E43q2_EoXgc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    dropout_layer,\n",
        "    #fc_layer,\n",
        "    prediction_layer\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQAQSwefSguR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
        "#opt = tf.keras.optimizers.Adam()\n",
        "#opt = tf.keras.optimizers.RMSprop()\n",
        "model.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDO3A5Mc4Dgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(batched_val_ds)\n",
        "print(\"Pre-training loss:\", loss)\n",
        "print(\"Pre-training accuracy:\", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4bZa2ADqtWt",
        "colab_type": "text"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGRsy5QxND_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This callback will stop the training when there is no improvement in\n",
        "# the validation loss for 'patience' consecutive epochs.\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                  patience=3)\n",
        "\n",
        "initial_epochs = 10\n",
        "spe = np.ceil(image_count * 0.7 / BATCH_SIZE)  # steps per epoch\n",
        "\n",
        "history = model.fit(batched_train_ds, \n",
        "                    epochs=initial_epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping],\n",
        "                    validation_data=batched_val_ds,\n",
        "                    steps_per_epoch=spe\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E_Jkenqoq0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the state of the model\n",
        "extraction_weights = model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-XJXQsO9Ujp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Valid'], loc='lower right')\n",
        "plt.savefig('fe_plot.pdf')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Valid'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBaVhJ-ssSvT",
        "colab_type": "text"
      },
      "source": [
        "### Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1q7WGbEBkQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_base_layers = len(base_model.layers)\n",
        "print(\"Number of layers in the base model:\", num_base_layers)\n",
        "# let's visualize layer names and layer indices to see how many layers\n",
        "# we should freeze:\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYCIbUAasXF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Un-freeze the top layers of the model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "#fine_tune_at = 974\n",
        "#fine_tune_at = 1004\n",
        "fine_tune_at = 1019\n",
        "#fine_tune_at = 1038\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RywpGL8EupCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load extraction weights (in case of layer freezing number tuning)\n",
        "model.set_weights(extraction_weights)\n",
        "\n",
        "# compile the model reducing learning rate \n",
        "ft_opt = tf.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "#ft_opt = tf.optimizers.Adam(learning_rate=0.0001)\n",
        "#ft_opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "\n",
        "model.compile(optimizer=ft_opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PcpSSskvRrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_epochs = 10\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(batched_train_ds,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=history.epoch[-1],\n",
        "                         verbose=1,\n",
        "                         callbacks=[early_stopping],\n",
        "                         validation_data=batched_val_ds,\n",
        "                         steps_per_epoch=spe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVgSi-6dwkJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
        "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss'] + history_fine.history['loss']\n",
        "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.ylim([0.7, 1.05])\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "          plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "         plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.savefig('plot.pdf')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcRGwqj4sFEH",
        "colab_type": "text"
      },
      "source": [
        "# Validation Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO--zokDryYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(batched_val_ds)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbvlC57tEWQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(batched_val_ds)\n",
        "\n",
        "if verbose:\n",
        "    print(\"Number of predictions:\", len(predictions))\n",
        "    print(predictions[0])\n",
        "    print(np.argmax(predictions[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oAXpOFqFQnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_batch_image(i, predictions_array, image_batch, label_batch):\n",
        "\n",
        "    predictions_array = predictions_array[i]\n",
        "    true_label = np.where(label_batch[i] == True)[0][0]\n",
        "    img = image_batch[i]\n",
        "\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.imshow(image_batch[i])\n",
        "  \n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    if predicted_label == true_label:\n",
        "        color = 'blue'\n",
        "    else:\n",
        "        color = 'red'\n",
        "  \n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,\n",
        "                                100 * np.max(predictions_array),\n",
        "                                true_label),\n",
        "                                color=color)\n",
        "    plt.ylabel(CLASS_NAMES[label_batch[i] == True][0])\n",
        "\n",
        "\n",
        "def plot_value_array(i, predictions_array, label_batch):\n",
        "    predictions_array = predictions_array[i]\n",
        "    true_label = np.where(label_batch[i] == True)[0][0]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(35), predictions_array, color=\"#777777\")\n",
        "    plt.ylim([0, 1]) \n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    \n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('blue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7702RabkFF0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the first X test images, their predicted label, and the true label\n",
        "# Color correct predictions in blue, incorrect predictions in red\n",
        "num_rows = 6\n",
        "num_cols = 5\n",
        "num_images = num_rows * num_cols\n",
        "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
        "image_batch, label_batch = next(batched_val_ds.as_numpy_iterator())\n",
        "for i in range(num_images):\n",
        "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
        "    plot_batch_image(i, predictions, image_batch, label_batch)\n",
        "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
        "    plot_value_array(i, predictions, label_batch)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qw3y4MATt5o",
        "colab_type": "text"
      },
      "source": [
        "# Test Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g0-XpL8prDr",
        "colab_type": "text"
      },
      "source": [
        "## Training on the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqPvaVj6p4y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_ds = prepare_for_training(labeled_ds, cache='training_ds.tfcache')\n",
        "\n",
        "# reinitialize the base model\n",
        "base_model.set_weights(imagenet_weights)\n",
        "base_model.trainable = False\n",
        "\n",
        "test_model = tf.keras.Sequential([\n",
        "                                  base_model,\n",
        "                                  global_average_layer,\n",
        "                                  dropout_layer,\n",
        "                                  #fc_layer,\n",
        "                                  prediction_layer\n",
        "                                  ])\n",
        "\n",
        "test_model.compile(optimizer=opt,\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX2Rd1n09sig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spe = np.ceil(image_count / BATCH_SIZE)\n",
        "\n",
        "test_history = test_model.fit(training_ds,\n",
        "                              epochs=initial_epochs,\n",
        "                              verbose=1,\n",
        "                              steps_per_epoch=spe\n",
        "                              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddDUsh_s9rID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "test_model.compile(optimizer=ft_opt,\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy']\n",
        "                   )\n",
        "print(\"Trainable variables:\", len(model.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF9GJ3kp_EKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_model.fit(training_ds,\n",
        "               epochs=total_epochs,\n",
        "               initial_epoch=test_history.epoch[-1],\n",
        "               verbose=1,\n",
        "               steps_per_epoch=spe\n",
        "               )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhUdpC0f_dG6",
        "colab_type": "text"
      },
      "source": [
        "##Load Test-Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4brjuRhHhVTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = \"/content/drive/My Drive/dogs/Dataset/test\"\n",
        "test_dir = pathlib.Path(test_dir)\n",
        "\n",
        "test_file_list = list(test_dir.glob('*.jpg'))\n",
        "\n",
        "test_image_count = len(test_file_list)\n",
        "if verbose: print(\"Number of test images:\", test_image_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnW4-FDop9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_ds_test = tf.data.Dataset.list_files(str(test_dir/'*'))\n",
        "# cache to save prediction ordering\n",
        "list_ds_test = list_ds_test.cache(\"list_ds_test.tfcache\")\n",
        "\n",
        "# cache check\n",
        "if verbose:\n",
        "    for f in list_ds_test.take(4):\n",
        "        print(f.numpy())\n",
        "    print(\"\\n\")\n",
        "    for f in list_ds_test.take(4):\n",
        "        print(f.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_0S5TWzo9He",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_test_path(file_path):\n",
        "    # load raw data from the file as a string\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = decode_img(img)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQbbgTQRtnso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ds = list_ds_test.map(process_test_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "if verbose:\n",
        "    for image in test_ds.take(1):\n",
        "        #print(\"Image tensor:\", image)\n",
        "        print(\"Image shape:\", image.numpy().shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRSKG-dpuLuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batched_test_ds = test_ds.batch(BATCH_SIZE).cache('./dogs-test.tfcache')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_B2NTxTBCgA",
        "colab_type": "text"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDSwkm8ruhJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = test_model.predict(batched_test_ds)\n",
        "#test_predictions = model.predict(batched_test_ds)\n",
        "\n",
        "print(\"Number of predictions:\", len(test_predictions))\n",
        "print(test_predictions[0])\n",
        "print(np.argmax(test_predictions[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipg-RaFuH5o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"test_results.csv\", \"w\")\n",
        "i = 0\n",
        "f.write(\"id,breed\\n\")\n",
        "for img_file in list_ds_test.as_numpy_iterator():\n",
        "    img_id = str(img_file).split(os.path.sep)[-1][:-5]\n",
        "    img_prediction = CLASS_NAMES[np.argmax(test_predictions[i])]\n",
        "    i += 1\n",
        "    f.write(img_id + \",\" + img_prediction + \"\\n\")\n",
        "print(\"predicion lines written:\", i)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}